# avance-tecnologico
Proyecto de la materia avance tecnológico iutepi

Inteligencia Artificial (IA) es un campo de las ciencias de la computación que busca crear sistemas y algoritmos capaces de imitar la inteligencia humana. Su objetivo es permitir que las máquinas realicen tareas que normalmente requerirían pensamiento, aprendizaje, resolución de problemas y toma de decisiones.

¿Qué es la IA en esencia?

En su núcleo, la IA combina datos, modelos matemáticos y capacidades computacionales para llevar a cabo tareas que antes requerían la intervención humana. Esto abarca desde el reconocimiento de imágenes y la generación de texto hasta la toma de decisiones estratégicas.

Tipos de IA:

Aunque existen diversas clasificaciones, se suele hablar de:

IA Débil (o estrecha): Diseñada para realizar tareas específicas, como los asistentes virtuales (Siri, Alexa), el reconocimiento facial o los sistemas de recomendación. Es la IA que existe actualmente.

IA Fuerte (o general - AGI): Aspira a emular la inteligencia humana en su totalidad, permitiendo a la máquina razonar, aprender y adaptarse como una persona. Todavía no existe.

Superinteligencia Artificial (ASI): Una IA que superaría la inteligencia humana en todos los aspectos, incluso en creatividad y sabiduría. Es un concepto futurista.

¿Cómo funciona la IA?

La IA se basa principalmente en el aprendizaje automático (Machine Learning) y el aprendizaje profundo (Deep Learning). Estos permiten a los sistemas aprender de grandes volúmenes de datos, identificar patrones y mejorar su rendimiento con el tiempo sin ser programados explícitamente para cada tarea.

Aplicaciones de la IA:

La IA está transformando numerosos sectores y aspectos de la vida cotidiana. Algunas de sus aplicaciones más destacadas incluyen:

Asistentes virtuales: Siri, Alexa, Google Assistant.

Automatización industrial y robótica: Optimización de la producción, mantenimiento predictivo, control de procesos.

Análisis de datos y predicción: Identificación de patrones y tendencias en marketing, detección de fraudes, modelado predictivo.

Conducción autónoma: Vehículos que pueden operar sin intervención humana.

Medicina y salud: Diagnósticos más rápidos y precisos, desarrollo de fármacos, cirugía robótica.

Servicios financieros: Detección de fraudes, análisis de riesgos, personalización de servicios.

Procesamiento de Lenguaje Natural (PLN): Traducción automática, chatbots, análisis de sentimientos.

Visión artificial: Reconocimiento de imágenes, reconocimiento facial, detección de objetos.

Creación de contenido: Generación de texto, imágenes, música.

Seguridad cibernética: Monitoreo y análisis de redes para detectar ataques.

Ventajas de la IA:

Automatización de procesos: Libera a los humanos de tareas repetitivas y tediosas.

Mayor precisión y reducción de errores: Supera la capacidad humana en el análisis de grandes volúmenes de datos y la detección de defectos.

Agiliza la toma de decisiones: Analiza datos a gran velocidad para ofrecer información valiosa.

Fomenta la creatividad: Permite a los humanos enfocarse en tareas más estratégicas y creativas.

Aumento de la productividad: Mejora la eficiencia en diversos sectores.

Personalización: Ofrece experiencias y servicios adaptados a cada usuario.

Desventajas y desafíos de la IA:

Alto costo de implementación: La inversión en tecnología, capacitación y adaptación de procesos puede ser elevada.

Falta de profesionales cualificados: Escasez de expertos en IA.

Dependencia tecnológica: Una excesiva dependencia puede llevar a la pérdida de habilidades humanas y a problemas si los sistemas fallan.

Historia de la IA

Precursores y Fundamentos (Antes de 1950):

Antigüedad y Renacimiento: La idea de construir seres artificiales inteligentes se remonta a mitos griegos, como el de Talos, un autómata gigante. Figuras como Ramon Llull en el siglo XIII y René Descartes en el XVII exploraron la lógica y la razón, sentando bases filosóficas.

Siglo XIX: Charles Babbage y Ada Lovelace sentaron las bases de la computación programable con su trabajo en la Máquina Analítica. George Boole desarrolló la lógica booleana, fundamental para la computación moderna.

Década de 1940:

1943: Warren McCulloch y Walter Pitts publican "A Logical Calculus of Ideas Immanent in Nervous Activity", un trabajo pionero que propone el primer modelo matemático de una neurona artificial, sentando las bases de las redes neuronales.

1950: Alan Turing publica "Computing Machinery and Intelligence", donde propone el famoso "Test de Turing" como una forma de evaluar si una máquina puede exhibir un comportamiento inteligente indistinguible del humano. Este artículo es considerado un punto de partida para la discusión teórica de la IA.

El Nacimiento de la IA como Campo (1956 - Década de 1970):

1956: Conferencia de Dartmouth: Este evento, organizado por John McCarthy, Marvin Minsky, Allen Newell y Herbert Simon, es ampliamente considerado el nacimiento oficial de la Inteligencia Artificial como disciplina académica. John McCarthy acuñó el término "Inteligencia Artificial" durante esta conferencia. Los investigadores se reunieron para explorar la posibilidad de construir máquinas que puedan simular aspectos de la inteligencia humana.

1957: Allen Newell y Herbert Simon desarrollan el General Problem Solver (GPS), un programa diseñado para imitar las estrategias humanas de resolución de problemas.

1958: Frank Rosenblatt crea el Perceptron Mark 1, la primera red neuronal artificial capaz de "aprender" a clasificar patrones mediante ensayo y error.

Décadas de 1960 y 1970:

1964-1966: Joseph Weizenbaum desarrolla ELIZA, uno de los primeros chatbots, que simulaba una conversación con un terapeuta mediante el uso de algoritmos simples de procesamiento de lenguaje natural.

1966: El robot Shakey de SRI International es uno de los primeros robots capaces de analizar su entorno y reaccionar a él.

Se produce un auge en la investigación de sistemas basados en el conocimiento y los "sistemas expertos", que intentaban codificar el conocimiento de expertos humanos para resolver problemas específicos.

Primer Invierno de la IA (Década de 1970 - 1980):

Las expectativas sobre la IA eran muy altas, pero los sistemas desarrollados en ese momento no podían cumplir con esas promesas.

La falta de poder computacional, la escasez de datos y las limitaciones en los algoritmos condujeron a una desaceleración en la financiación y el entusiasmo por la IA.

El Resurgimiento de la IA y el Aprendizaje Automático (Década de 1980 - 2000):

Década de 1980: Resurge el interés en las redes neuronales con el desarrollo del algoritmo de retropropagación (backpropagation) por Rumelhart, Hinton y Williams en 1986, lo que permitió entrenar redes neuronales multicapa de manera más eficiente.

Década de 1990:

1997: Deep Blue de IBM, un programa de ajedrez, derrota al campeón mundial Garry Kasparov, marcando un hito significativo en la capacidad de las máquinas para superar a los humanos en tareas intelectuales específicas.

Crecimiento del Machine Learning (Aprendizaje Automático) como subcampo de la IA, enfocado en el desarrollo de algoritmos que permiten a las computadoras aprender de los datos sin ser programadas explícitamente.

El Auge del Aprendizaje Profundo y la IA Contemporánea (2000 - Presente):

Principios de los 2000: La IA comienza a infiltrarse en la vida cotidiana con aplicaciones en motores de búsqueda, sistemas de recomendación y procesamiento de lenguaje natural.

2011: IBM Watson gana el concurso de preguntas y respuestas "Jeopardy!", demostrando una capacidad avanzada de procesamiento de lenguaje natural y razonamiento.

2012 en adelante: El auge del Deep Learning (Aprendizaje Profundo):

Gracias al aumento masivo de datos (Big Data) y al poder computacional (incluyendo las GPU), las redes neuronales profundas experimentan un resurgimiento y logran resultados impresionantes en tareas como el reconocimiento de imágenes, el reconocimiento de voz y el procesamiento de lenguaje natural.

2012: Andrew Ng y otros investigadores logran entrenar una red neuronal con 10 millones de vídeos de YouTube, lo que demuestra la capacidad de las redes neuronales profundas para aprender características complejas de grandes conjuntos de datos.

2016: AlphaGo de DeepMind (Google) derrota al campeón mundial de Go, Lee Sedol, un juego considerado mucho más complejo para la IA que el ajedrez.

Finales de la década de 2010 y principios de 2020: IA Generativa y Modelos de Lenguaje Grande (LLMs):

Desarrollo de arquitecturas como los Transformers ha revolucionado el Procesamiento del Lenguaje Natural, llevando a la creación de modelos de lenguaje grande (LLMs) como GPT (Generative Pre-trained Transformer) de OpenAI.

2022-2023: El lanzamiento de ChatGPT y otras herramientas de IA generativa (como generadores de imágenes) al público masivo marca un punto de inflexión, mostrando la capacidad de la IA para generar contenido creativo y mantener conversaciones complejas.

IOT

IOT

El Internet de las Cosas (IoT) es una vasta red de objetos físicos (las "cosas") que están equipados con sensores, software y otras tecnologías. Estos dispositivos tienen la capacidad de conectarse e intercambiar datos con otros dispositivos y sistemas a través de internet o de otras redes de comunicación.

¿Cómo funciona el IoT?
Un sistema IoT funciona en un ciclo continuo de recolección, transmisión, procesamiento y acción de datos:

Dispositivos y sensores: En la base del IoT están los objetos cotidianos, desde un termostato inteligente hasta un rastreador de actividad física o una máquina industrial. Estos objetos tienen sensores que recogen diversos tipos de datos de su entorno o de las interacciones del usuario (como temperatura, movimiento, luz, presión o ubicación).

Conectividad: Una vez que se recogen los datos, deben enviarse a un sistema central para su procesamiento. Los dispositivos IoT utilizan varios protocolos de comunicación (como Wi-Fi, Bluetooth, celular (4G/5G), Zigbee o LoRaWAN) para conectarse a internet o directamente a otros dispositivos. La elección de la conectividad depende de factores como el alcance, el consumo de energía y la velocidad de transferencia de datos.

Procesamiento de datos: Los datos recogidos, a menudo en volúmenes masivos, se transmiten a servidores basados en la nube o a dispositivos de computación edge locales. Aquí, los datos se procesan, analizan e integran con aplicaciones de software. Este procesamiento puede implicar desde una simple agregación de datos hasta análisis complejos o la aplicación de algoritmos de aprendizaje automático para obtener insights.

Interfaz de usuario/Acción: Finalmente, los datos procesados se utilizan para informar a los usuarios o para activar acciones automatizadas. Esto puede ser a través de una aplicación de smartphone que te permite controlar tus dispositivos domésticos inteligentes, un panel de control que proporciona información sobre el rendimiento de equipos industriales, o un sistema automatizado que ajusta el clima de tu coche según las condiciones externas.

¿Por qué es tan importante el IoT?
El IoT se ha convertido en una de las tecnologías más transformadoras del siglo XXI porque tiende un puente entre el mundo físico y el digital. Al permitir que los objetos cotidianos recojan y compartan datos con una mínima intervención humana, el IoT crea nuevas posibilidades para:

Automatización: Automatizar tareas y procesos rutinarios.

Eficiencia: Optimizar el uso de recursos y reducir el desperdicio.

Información: Obtener datos en tiempo real para una mejor toma de decisiones.

Comodidad: Hacer la vida diaria más cómoda y receptiva.

Nuevos servicios: Crear modelos de negocio y ofertas de servicios completamente nuevos.

Aplicaciones comunes del IoT
El IoT está revolucionando varios sectores:

Hogares inteligentes: Dispositivos como termostatos inteligentes, sistemas de iluminación, cámaras de seguridad y electrodomésticos conectados que se pueden controlar a distancia para mayor comodidad, eficiencia energética y seguridad.

Ciudades inteligentes: Uso de sensores para gestionar el flujo de tráfico, optimizar el alumbrado público, monitorear la calidad del aire, gestionar residuos y mejorar la seguridad pública.

Salud (IoMT - Internet de las Cosas Médicas): Rastreadores de actividad física, dispositivos de monitoreo remoto de pacientes, equipos hospitalarios inteligentes para mejores diagnósticos, atención personalizada y operaciones eficientes.

IoT Industrial (IIoT): Sensores y máquinas conectadas en la fabricación, la logística y las cadenas de suministro para el mantenimiento predictivo, el seguimiento de activos, el control de calidad y la optimización de la producción.

Agricultura inteligente: Sensores que monitorean la humedad del suelo, la salud de los cultivos y la ubicación del ganado para optimizar el riego, la fertilización y la gestión de recursos.

Vehículos conectados: Coches que recogen datos sobre el rendimiento, el tráfico y el comportamiento del conductor para mejorar la seguridad, la navegación y permitir funciones de conducción autónoma.

Comercio minorista: Estanterías inteligentes para la gestión de inventario, ofertas personalizadas a los clientes y optimización de la disposición de las tiendas según el comportamiento del cliente.

Beneficios del IoT
Reducción de costos: Optimización del uso de recursos, automatización de tareas y habilitación del mantenimiento predictivo para prevenir averías costosas.

Mayor eficiencia y productividad: Agilización de procesos, reducción del trabajo manual y monitoreo en tiempo real de las operaciones.

Mejor toma de decisiones: Proporciona grandes cantidades de datos en tiempo real para decisiones más informadas y estratégicas.

Experiencia del cliente mejorada: Ofrece servicios y productos personalizados, y aborda proactivamente los problemas antes de que los clientes los noten.

Mayor seguridad: Monitoreo de entornos para detectar peligros, permite la vigilancia remota y automatiza las respuestas de emergencia.

Desafíos del IoT
A pesar de su inmenso potencial, el IoT enfrenta varios desafíos importantes:

Seguridad y privacidad: El gran número de dispositivos conectados y los datos sensibles que recogen crean numerosos puntos de entrada para los ciberataques y plantean importantes preocupaciones sobre la privacidad. Las contraseñas predeterminadas, las vulnerabilidades no parcheadas y las interfaces inseguras son problemas comunes.

Interoperabilidad y estandarización: Con innumerables fabricantes y diferentes protocolos de comunicación, lograr que los dispositivos de varios proveedores se comuniquen y trabajen juntos sin problemas sigue siendo un gran obstáculo.

Gestión de datos: El enorme volumen, la velocidad y la variedad de datos generados por los dispositivos IoT presentan desafíos para el almacenamiento, el procesamiento y el análisis, lo que requiere una infraestructura robusta y análisis avanzados.

Escalabilidad: A medida que el número de dispositivos IoT sigue creciendo exponencialmente, escalar las infraestructuras IoT para manejar este aumento masivo de conectividad y datos se vuelve cada vez más complejo.

Complejidad: Diseñar, implementar y gestionar ecosistemas IoT robustos requiere habilidades técnicas especializadas y puede ser un desafío técnico.

Preocupaciones éticas: Surgen preguntas sobre la propiedad de los datos, el sesgo algorítmico, el potencial de desplazamiento de empleos debido a la automatización y el impacto ambiental de la fabricación y eliminación de dispositivos.

Riesgos de privacidad y seguridad: El manejo de grandes volúmenes de datos sensibles plantea desafíos éticos y de seguridad.

Sesgos en los datos: Si los datos de entrenamiento están sesgados, la IA puede reproducir y amplificar esos sesgos.

Implicaciones éticas: Cuestiones sobre la responsabilidad, el impacto en el empleo y el uso de la IA para fines no deseados.


